name: MLOps ETL Pipeline - Extract, Transform, Load
description: >
  This project implements a complete Extract-Transform-Load (ETL) pipeline for preparing agricultural machine learning data.
  It runs on a GPU node with persistent storage mounted at /mnt/swift_store and processes HRRR weather and USDA yield data.

sections:
  - title: Directory Structure
    content: |
      ```
      etl_pipeline/
      â”œâ”€â”€ extract/
      â”‚   â”œâ”€â”€ extract.py           # Downloads & cleans raw HRRR/USDA data
      â”‚   â””â”€â”€ Dockerfile           # Container to run the extract job
      â”œâ”€â”€ transform/
      â”‚   â”œâ”€â”€ transform.py         # Parses, merges, and outputs structured datasets
      â”‚   â””â”€â”€ Dockerfile           # Container to run the transform/load job
      â”œâ”€â”€ docker-compose.yml       # Defines extract and transform services
      â””â”€â”€ /mnt/swift_store/        # Mounted persistent volume (on GPU node)
          â”œâ”€â”€ raw_data/            # Output from extract
          â””â”€â”€ transformed_data/    # Output from transform/load
      ```

  - title: What the Pipeline Does
    content: |
      ### Extract Phase
      - Downloads HRRR and USDA datasets using `gdown` from Google Drive.
      - Filters out unwanted years (keeps 2017â€“2021).
      - Stores files to `/mnt/swift_store/raw_data/`.

      ### Transform Phase
      - Aggregates HRRR weather data by FIPS and date.
      - Dynamically identifies and merges USDA yield data by FIPS/year.
      - Keeps everything in memory for efficiency.

      ### Load Phase
      - Saves clean output datasets to `/mnt/swift_store/transformed_data/`:
        - `training.csv` â€” for model training (2017â€“2020)
        - `test.csv` â€” for evaluation (2021 even days)
        - `production/` â€” split `val` (2021 odd days) into:
          - `dev.csv`
          - `staging.csv`
          - `canary.csv`

  - title: âš™ï¸ Environment Setup (GPU Node)
    content: |
      ### 1. SSH into the GPU node
      ```bash
      ssh -i ~/.ssh/mlops_proj_key cc@A.B.C.D
      ```

      ### 2. Create a `.env` file
      ```bash
      cd ~/etl_pipeline
      nano .env
      ```

      Add folder IDs:
      ```env
      GOOGLE_DRIVE_FOLDER_ID_HRRR=your_hrrr_folder_id
      GOOGLE_DRIVE_FOLDER_ID_USDA=your_usda_folder_id
      ```

  - title: Running the Pipeline (Docker Compose)
    content: |
      ### Build Services
      ```bash
      docker compose build etl-download etl-transform
      ```

      ### Run Extract Phase
      ```bash
      docker compose run etl-download
      ```

      ### Run Transform + Load Phase
      ```bash
      docker compose run etl-transform
      ```

  - title: Move Production Data to Services Node
    content: |
      From the GPU node, run the following to transfer production files:
      ```bash
      scp -i ~/.ssh/mlops_proj_key /mnt/swift_store/transformed_data/production/*.csv \
        cc@<SERVICES_NODE_IP>:/mnt/swift_store/env_split/val_2022/
      ```

      Replace `<SERVICES_NODE_IP>` with the IP address of  services VM.

  
  - title: ğŸ§¹ Delete Production Data from GPU After Verification
    content: |
      After verifying the files are present and correct on the services node,
      remove the production data from the GPU node to prevent duplication and save space:

      ```bash
      rm -rf /mnt/swift_store/transformed_data/production
      ```

  - title: Outputs
    content: |
      After successful execution:
      ```
      /mnt/swift_store/
      â”œâ”€â”€ raw_data/
      â”‚   â”œâ”€â”€ WRF-HRRR Computed dataset/
      â”‚   â””â”€â”€ USDA Crop Dataset/
      â””â”€â”€ transformed_data/
          â”œâ”€â”€ training.csv
          â”œâ”€â”€ test.csv
      ```

  - title: Requirements
    content: |
      - Python 3.8+
      - `gdown`, `pandas`
      - Docker + Docker Compose
      - Mounted volume at `/mnt/swift_store`

  - title: Summary
    content: |
      | Step     | Input                      | Output                                  |
      |----------|-----------------------------|-----------------------------------------|
      | Extract  | Google Drive folders        | `/mnt/swift_store/raw_data/`           |
      | Transform| Raw HRRR & USDA data        | In-memory merge                         |
      | Load     | Transformed merged dataset  | `training.csv`, `test.csv`, prod splits |