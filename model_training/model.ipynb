{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# put actual numpy arrays here please\n",
    "# X.shape == (num_samples, seq_len=73, num_features)\n",
    "# y.shape == (num_samples,)\n",
    "X = np.random.randn(500, 73, 5)   # example: 500 countyâ€‘years, 5 features \n",
    "y = np.random.randn(500)         # example yield values\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db98e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 32\n",
    "dataset = CropDataset(X, y)\n",
    "loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd3dbc",
   "metadata": {},
   "source": [
    "### LSTM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim,\n",
    "                            num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, F]\n",
    "        _, (h_n, _) = self.lstm(x)        # h_n: [num_layers, B, hidden_dim]\n",
    "        out = self.fc(h_n[-1])            # take last layer's hidden state\n",
    "        return out.squeeze(1)             # [B]\n",
    "\n",
    "# instantiate\n",
    "input_dim = X.shape[-1]\n",
    "lstm_model = LSTMRegressor(input_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c72de0",
   "metadata": {},
   "source": [
    "### TCN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868aa3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.utils.weight_norm(\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size,\n",
    "                      stride=stride, padding=padding, dilation=dilation)\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.utils.weight_norm(\n",
    "            nn.Conv1d(out_ch, out_ch, kernel_size,\n",
    "                      stride=stride, padding=padding, dilation=dilation)\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.downsample = (\n",
    "            nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else None\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu2(out + res)\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            in_ch = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_ch = num_channels[i]\n",
    "            dilation = 2 ** i\n",
    "            padding  = (kernel_size-1) * dilation\n",
    "            layers.append(\n",
    "                TemporalBlock(in_ch, out_ch, kernel_size,\n",
    "                              stride=1, dilation=dilation,\n",
    "                              padding=padding, dropout=dropout)\n",
    "            )\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, F] -> [B, F, T] for Conv1d\n",
    "        x = x.transpose(1, 2)\n",
    "        y = self.network(x)\n",
    "        # y: [B, C_last, T] -> take last time step\n",
    "        return y[:, :, -1]\n",
    "\n",
    "class TCNRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, channel_sizes=[32, 32, 64], kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.tcn = TCN(input_dim, channel_sizes, kernel_size=kernel_size)\n",
    "        self.fc  = nn.Linear(channel_sizes[-1], 1)\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, F]\n",
    "        y = self.tcn(x)     # [B, C_last]\n",
    "        return self.fc(y).squeeze(1)\n",
    "\n",
    "# instantiate\n",
    "tcn_model = TCNRegressor(input_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643c390",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, lr=1e-3, epochs=20):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    for ep in range(1, epochs+1):\n",
    "        total_loss = 0\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred   = model(xb)\n",
    "            loss   = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "        avg_loss = total_loss / len(loader.dataset)\n",
    "        print(f\"{model.__class__.__name__} | Epoch {ep:02d} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Train LSTM\n",
    "train_model(lstm_model, loader, lr=1e-3, epochs=10)\n",
    "# Train TCN\n",
    "train_model(tcn_model, loader, lr=1e-3, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05d9fb",
   "metadata": {},
   "source": [
    "quick eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xb = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        yb = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "        preds = model(xb)\n",
    "        mse   = nn.MSELoss()(preds, yb).item()\n",
    "    print(f\"{model.__class__.__name__} final MSE: {mse:.4f}\")\n",
    "\n",
    "evaluate(lstm_model)\n",
    "evaluate(tcn_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
