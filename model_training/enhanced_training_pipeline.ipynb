{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRvMPxF7zLWa"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CropYieldDataset(Dataset):\n",
        "    def __init__(self, data_lake_dir=\"../data/data_lake_organized\", crop_name=\"corn\", transform=None):\n",
        "        self.samples = [] # Stores (weather_tensor, yield_target, fips_id)\n",
        "        self.transform = transform\n",
        "        self.crop_name = crop_name.lower()\n",
        "        self.fips_to_id = {} # Map FIPS string to integer ID\n",
        "        self.id_to_fips = {} # Map integer ID back to FIPS string\n",
        "        self._next_fips_id = 0\n",
        "\n",
        "        fips_folders = [f for f in Path(data_lake_dir).iterdir() if f.is_dir()]\n",
        "\n",
        "        for fips_folder in fips_folders:\n",
        "            fips_code = fips_folder.name # The folder name is the FIPS code\n",
        "\n",
        "            # Get or assign unique integer ID for this FIPS\n",
        "            if fips_code not in self.fips_to_id:\n",
        "                self.fips_to_id[fips_code] = self._next_fips_id\n",
        "                self.id_to_fips[self._next_fips_id] = fips_code\n",
        "                self._next_fips_id += 1\n",
        "\n",
        "            fips_id = self.fips_to_id[fips_code]\n",
        "\n",
        "            crop_json_path = fips_folder / f\"{self.crop_name}.json\"\n",
        "            if not crop_json_path.exists():\n",
        "                continue\n",
        "\n",
        "            with open(crop_json_path, 'r') as f:\n",
        "                yield_data = json.load(f)\n",
        "\n",
        "            year_folders = [y for y in fips_folder.iterdir() if y.is_dir()]\n",
        "\n",
        "            for year_folder in year_folders:\n",
        "                year_str = year_folder.name # Year is a string from folder name\n",
        "                weather_csv = year_folder / f\"WeatherTimeSeries{year_str}.csv\"\n",
        "\n",
        "                # Ensure year_str exists in yield_data before processing weather\n",
        "                if year_str not in yield_data:\n",
        "                     # print(f\"Warning: No yield data for year {year_str} in FIPS {fips_code}\") # Optional warning\n",
        "                     continue\n",
        "\n",
        "                if not weather_csv.exists():\n",
        "                    # print(f\"Warning: Weather CSV missing for FIPS {fips_code}, year {year_str}\") # Optional warning\n",
        "                    continue\n",
        "\n",
        "                df = pd.read_csv(weather_csv)\n",
        "\n",
        "                # Only keep Aprilâ€“October (assuming your training data is always this range)\n",
        "                df = df[(df['Month'] >= 4) & (df['Month'] <= 10)]\n",
        "\n",
        "                # Drop non-weather columns\n",
        "                # Be careful: Make sure 'Year', 'Month', 'Day' columns exist before dropping\n",
        "                cols_to_drop = ['Year', 'Month', 'Day']\n",
        "                existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
        "                df = df.drop(columns=existing_cols_to_drop, errors='ignore')\n",
        "\n",
        "                # Make sure it's float tensor\n",
        "                weather_tensor = torch.tensor(df.values, dtype=torch.float32)\n",
        "\n",
        "                # Target\n",
        "                yield_target = torch.tensor(yield_data[year_str]['yield'], dtype=torch.float32)\n",
        "\n",
        "                # Store sample as (weather_tensor, yield_target, fips_id)\n",
        "                self.samples.append((weather_tensor, yield_target, fips_id))\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} samples for crop '{self.crop_name}'. Found {len(self.fips_to_id)} unique FIPS codes.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return (weather_tensor, yield_target, fips_id)\n",
        "        x, y, fips = self.samples[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x) # Apply transform only to weather data\n",
        "        return x, y, fips\n",
        "\n",
        "    def get_fips_mapping(self):\n",
        "        return self.fips_to_id, self.id_to_fips\n",
        "\n",
        "    def get_num_fips(self):\n",
        "        return len(self.fips_to_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTCNRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, num_fips, fips_embedding_dim=16, hidden_dim=64, lstm_layers=1, tcn_channels=[64, 32], dropout_rate=0.1):\n",
        "        super(LSTMTCNRegressor, self).__init__()\n",
        "\n",
        "        self.fips_embedding = nn.Embedding(num_fips, fips_embedding_dim)\n",
        "\n",
        "        # Input to LSTM will be weather_features + fips_embedding_dim\n",
        "        lstm_input_dim = input_dim + fips_embedding_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(lstm_input_dim, hidden_dim, num_layers=lstm_layers, batch_first=True, dropout=dropout_rate if lstm_layers > 1 else 0)\n",
        "\n",
        "        # TCN part: using 1D Convolutions\n",
        "        tcn_layers = []\n",
        "        in_channels = hidden_dim\n",
        "        for out_channels in tcn_channels:\n",
        "             tcn_layers.append(nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1))\n",
        "             tcn_layers.append(nn.ReLU())\n",
        "             in_channels = out_channels\n",
        "\n",
        "        self.tcn = nn.Sequential(*tcn_layers)\n",
        "\n",
        "        # Adaptive pooling after TCN\n",
        "        self.pooling = nn.AdaptiveAvgPool1d(1) # output shape: (batch, channels, 1)\n",
        "\n",
        "        # Final fully connected layer\n",
        "        self.fc = nn.Linear(tcn_channels[-1], 1) # Outputting 1 value for point estimate\n",
        "\n",
        "        self.dropout_rate = dropout_rate # Store dropout rate\n",
        "\n",
        "    def forward(self, x_weather, fips_ids):\n",
        "        # x_weather shape: (batch, time_steps, weather_features)\n",
        "        # fips_ids shape: (batch,)\n",
        "\n",
        "        # Get county embeddings and unsqueeze for concatenation\n",
        "        fips_emb = self.fips_embedding(fips_ids) # (batch, fips_embedding_dim)\n",
        "        # Repeat embedding across the time dimension to match weather sequence length\n",
        "        # (batch, 1, fips_embedding_dim) -> (batch, time_steps, fips_embedding_dim)\n",
        "        fips_emb = fips_emb.unsqueeze(1).repeat(1, x_weather.size(1), 1)\n",
        "\n",
        "        # Concatenate weather features and county embeddings\n",
        "        x_combined = torch.cat([x_weather, fips_emb], dim=-1) # (batch, time_steps, weather_features + fips_embedding_dim)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        # out shape: (batch, time_steps, hidden_dim)\n",
        "        out, _ = self.lstm(x_combined)\n",
        "\n",
        "        # Apply dropout to LSTM output if needed (LSTM layer handles this if num_layers > 1)\n",
        "        # If dropout was applied internally by LSTM, we don't need another one here usually.\n",
        "        # If lstm_layers == 1 and dropout_rate > 0, you might add nn.Dropout(self.dropout_rate)(out) here\n",
        "        # But let's rely on LSTM's built-in dropout for simplicity if layers > 1\n",
        "\n",
        "        # Permute for Conv1D\n",
        "        out = out.permute(0, 2, 1)  # (batch, hidden_dim, time_steps)\n",
        "\n",
        "        # Pass through TCN\n",
        "        out = self.tcn(out) # (batch, tcn_channels[-1], time_steps_after_conv) - padding=1 keeps length\n",
        "\n",
        "        # Apply pooling\n",
        "        out = self.pooling(out)  # (batch, tcn_channels[-1], 1)\n",
        "\n",
        "        # Squeeze the last dimension\n",
        "        out = out.squeeze(-1)  # (batch, tcn_channels[-1])\n",
        "\n",
        "        # Pass through final FC layer\n",
        "        out = self.fc(out)  # (batch, 1)\n",
        "\n",
        "        return out.squeeze(-1) # Output (batch,)"
      ],
      "metadata": {
        "id": "xQO6O40Tze6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    # batch is a list of (weather_tensor, yield_target, fips_id) tuples\n",
        "    xs = [item[0] for item in batch] # weather_tensor\n",
        "    ys = [item[1] for item in batch] # yield_target\n",
        "    fips_ids = [item[2] for item in batch] # fips_id\n",
        "\n",
        "    # Pad weather sequences\n",
        "    xs_padded = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0) # Choose a padding_value (0 is common for weather)\n",
        "\n",
        "    # Stack targets and fips_ids\n",
        "    ys_stacked = torch.stack(ys)\n",
        "    fips_ids_stacked = torch.stack(fips_ids)\n",
        "\n",
        "    return xs_padded, ys_stacked, fips_ids_stacked"
      ],
      "metadata": {
        "id": "LPig-TEUzdOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def get_dataloaders(dataset, train_ratio=0.8, batch_size=32):\n",
        "    \"\"\"\n",
        "    Splits the dataset into training and validation sets and returns DataLoaders.\n",
        "    Includes the custom collate_fn.\n",
        "    \"\"\"\n",
        "    total_len = len(dataset)\n",
        "    train_len = int(total_len * train_ratio)\n",
        "    val_len = total_len - train_len\n",
        "\n",
        "    # Ensure split is valid\n",
        "    if train_len == 0 or val_len == 0:\n",
        "        raise ValueError(\"Dataset too small for train/validation split.\")\n",
        "\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=30, lr=1e-3):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for x_batch, y_batch, fips_batch in train_loader:\n",
        "            x_batch, y_batch, fips_batch = x_batch.to(device), y_batch.to(device), fips_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x_batch, fips_batch) # Pass FIPS IDs\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Validation loss\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val, fips_val in val_loader:\n",
        "                x_val, y_val, fips_val = x_val.to(device), y_val.to(device), fips_val.to(device)\n",
        "                y_pred = model(x_val, fips_val) # Pass FIPS IDs\n",
        "                val_loss += criterion(y_pred, y_val).item()\n",
        "\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        tqdm.tqdm.write(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
        "        tqdm.tqdm.set_description(f\"Training Progress (Epoch {epoch+1} Val Loss: {avg_val_loss:.4f})\")\n",
        "\n",
        "        # Simple early stopping or saving best model based on validation loss\n",
        "        if avg_val_loss < best_val_loss:\n",
        "             best_val_loss = avg_val_loss\n",
        "             # You might want to save the model state here: torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    return model # Return the trained model\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval() # Set model to evaluation mode (dropout is off by default)\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val, fips_val in val_loader: # Unpack FIPS IDs\n",
        "            x_val, y_val, fips_val = x_val.to(device), y_val.to(device), fips_val.to(device)\n",
        "            outputs = model(x_val, fips_val) # Pass FIPS IDs\n",
        "            y_true.extend(y_val.cpu().numpy())\n",
        "            y_pred.extend(outputs.cpu().numpy())\n",
        "\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    print(f\"Final Validation RMSE: {rmse:.4f}\")\n",
        "    print(f\" Final Validation MAE: {mae:.4f}\")\n",
        "\n",
        "    return rmse, mae"
      ],
      "metadata": {
        "id": "2CTh2TuCzgaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the dataset\n",
        "dataset = CropYieldDataset(crop_name=\"corn\")\n",
        "\n",
        "# Get county mapping and number of unique counties\n",
        "fips_to_id, id_to_fips = dataset.get_fips_mapping()\n",
        "num_fips = dataset.get_num_fips()\n",
        "\n",
        "# Get dataloaders\n",
        "# Add try-except in case the dataset is too small after filtering\n",
        "try:\n",
        "    train_loader, val_loader = get_dataloaders(dataset, batch_size=32)\n",
        "except ValueError as e:\n",
        "    print(f\"Error creating dataloaders: {e}\")\n",
        "    print(\"Dataset may be too small for split after filtering.\")\n",
        "    # Handle this case, maybe exit or use a different strategy\n",
        "\n",
        "# Detect input_dim (number of weather features)\n",
        "# Ensure the loader has data before accessing shape\n",
        "input_dim = -1\n",
        "if len(dataset) > 0:\n",
        "    sample_weather, _, _ = dataset[0]\n",
        "    input_dim = sample_weather.shape[-1]\n",
        "else:\n",
        "    print(\"Dataset is empty. Cannot determine input_dim.\")\n",
        "    # Handle empty dataset case\n",
        "\n",
        "if input_dim != -1:\n",
        "    # Instantiate the model with num_fips and embedding dim\n",
        "    # You can add fips_embedding_dim to your hyperparameters if tuning\n",
        "    model = LSTMTCNRegressor(input_dim=input_dim, num_fips=num_fips, fips_embedding_dim=16) # Choose an embedding dim\n",
        "\n",
        "    # Train the model\n",
        "    # Pass loaders, num_epochs, lr\n",
        "    trained_model = train_model(model, train_loader, val_loader, num_epochs=30)\n",
        "\n",
        "    # Evaluate\n",
        "    evaluate_model(trained_model, val_loader)\n",
        "\n",
        "    # Hyperparameter tuning (needs update to pass num_fips and embedding_dim)\n",
        "    # The random search function would need modification similar to train_model\n",
        "    # to pass fips_ids and instantiate the model with num_fips and fips_embedding_dim\n",
        "    # For brevity, I won't rewrite the whole tuning function here, but the principle is the same.\n",
        "    # You'd need to add num_fips and fips_embedding_dim to the model instantiation inside the loop.\n",
        "    # Example inside the loop:\n",
        "    # model = LSTMTCNRegressor(input_dim=input_dim, num_fips=num_fips, fips_embedding_dim=random.choice([8, 16, 32]), ...)\n",
        "    # trained_model = train_model(model, train_loader, val_loader, num_epochs=num_epochs, lr=lr)"
      ],
      "metadata": {
        "id": "w1PYPKpIzh3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Optional: for visualization\n",
        "\n",
        "\n",
        "def predict_distribution(\n",
        "    model,\n",
        "    data_lake_dir,\n",
        "    year,\n",
        "    county_fips,\n",
        "    crop_name,\n",
        "    inference_date, # Example: '2023-05-15'\n",
        "    fips_to_id_mapping,\n",
        "    num_samples=30, # Number of runs for dropout sampling\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Predicts a yield distribution for a specific year, county, crop, and date\n",
        "    using a trained model with dropout enabled at inference.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained model.\n",
        "        data_lake_dir (str): Path to the data lake root.\n",
        "        year (int or str): The year for prediction.\n",
        "        county_fips (str): The FIPS code for the county.\n",
        "        crop_name (str): The name of the crop.\n",
        "        inference_date (str): The date up to which weather data is available (YYYY-MM-DD).\n",
        "        fips_to_id_mapping (dict): Dictionary mapping FIPS codes (str) to integer IDs.\n",
        "        num_samples (int): Number of forward passes with dropout to sample the distribution.\n",
        "        device (torch.device, optional): The device to use for inference. Defaults to cpu/cuda.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (predicted_yields_samples, bins, hist_counts)\n",
        "               predicted_yields_samples (list): List of individual predicted yields.\n",
        "               bins (numpy.ndarray): Histogram bin edges.\n",
        "               hist_counts (numpy.ndarray): Histogram counts per bin.\n",
        "    \"\"\"\n",
        "    year_str = str(year)\n",
        "    crop_name_lower = crop_name.lower()\n",
        "    device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --- 1. Load Partial Weather Data ---\n",
        "    fips_folder = Path(data_lake_dir) / county_fips\n",
        "    year_folder = fips_folder / year_str\n",
        "    weather_csv = year_folder / f\"WeatherTimeSeries{year_str}.csv\"\n",
        "\n",
        "    if not weather_csv.exists():\n",
        "        print(f\"Error: Weather CSV not found for FIPS {county_fips}, year {year_str}\")\n",
        "        return None, None, None\n",
        "\n",
        "    df_weather = pd.read_csv(weather_csv)\n",
        "\n",
        "    # Filter weather data up to the inference date\n",
        "    try:\n",
        "        inference_date_dt = pd.to_datetime(inference_date)\n",
        "        df_weather['Date'] = pd.to_datetime(df_weather[['Year', 'Month', 'Day']])\n",
        "        df_partial_weather = df_weather[df_weather['Date'] <= inference_date_dt].copy()\n",
        "        # Drop the added 'Date' column and other non-weather columns\n",
        "        cols_to_drop = ['Year', 'Month', 'Day', 'Date']\n",
        "        existing_cols_to_drop = [col for col in cols_to_drop if col in df_partial_weather.columns]\n",
        "        df_partial_weather = df_partial_weather.drop(columns=existing_cols_to_drop, errors='ignore')\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing dates: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    if df_partial_weather.empty:\n",
        "        print(f\"Error: No weather data found up to {inference_date} for FIPS {county_fips}, year {year_str}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Convert to tensor\n",
        "    weather_tensor = torch.tensor(df_partial_weather.values, dtype=torch.float32).unsqueeze(0).to(device) # Add batch dimension (1, T, features)\n",
        "\n",
        "    # --- 2. Get County ID ---\n",
        "    if county_fips not in fips_to_id_mapping:\n",
        "        print(f\"Error: County FIPS {county_fips} not found in the training mapping.\")\n",
        "        print(\"Model was not trained on this county.\")\n",
        "        # Option: Fallback to a default county embedding, or raise error\n",
        "        return None, None, None\n",
        "\n",
        "    fips_id = fips_to_id_mapping[county_fips]\n",
        "    fips_tensor = torch.tensor([fips_id], dtype=torch.long).to(device) # (1,) tensor\n",
        "\n",
        "    # --- 3. Run Model Multiple Times with Dropout ---\n",
        "    model.eval() # Start in eval mode\n",
        "    # Temporarily enable dropout for inference\n",
        "    def enable_dropout(m):\n",
        "        if type(m) == nn.Dropout:\n",
        "            m.train()\n",
        "        if type(m) == nn.LSTM: # LSTMs with dropout need .train() to apply dropout\n",
        "             m.train() # Use .train() to enable dropout in LSTM/GRU\n",
        "             # Need to restore state later\n",
        "             # A safer way is to manually set dropout probability during inference\n",
        "             # Let's stick to m.train() for simplicity first, but be aware\n",
        "             # A more robust way involves a custom forward pass or modifying the module.\n",
        "             # For LSTMs, .train() is the standard way to enable recurrent dropout.\n",
        "\n",
        "    model.apply(enable_dropout) # Apply enable_dropout function recursively to all submodules\n",
        "\n",
        "    predicted_yields = []\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        for _ in range(num_samples):\n",
        "            # Pass weather and fips tensor (batch size 1)\n",
        "            prediction = model(weather_tensor, fips_tensor)\n",
        "            predicted_yields.append(prediction.item()) # Get the scalar prediction\n",
        "\n",
        "    # Revert model back to standard eval mode (dropout off)\n",
        "    model.eval()\n",
        "\n",
        "    # --- 4. Generate Histogram Data ---\n",
        "    # Determine bins (e.g., based on min/max predictions, or historical range if available)\n",
        "    # For a simple approach, use the min/max of the sampled predictions\n",
        "    min_yield = min(predicted_yields)\n",
        "    max_yield = max(predicted_yields)\n",
        "    # Create bins - e.g., 20 bins between min and max\n",
        "    bins = np.linspace(min_yield, max_yield, 20) # Adjust number of bins as needed\n",
        "\n",
        "    hist_counts, bin_edges = np.histogram(predicted_yields, bins=bins, density=False) # Use density=False for counts\n",
        "\n",
        "    # You can normalize counts to get probabilities if needed: hist_probabilities = hist_counts / num_samples\n",
        "\n",
        "    print(f\"\\nInference for {crop_name} in FIPS {county_fips}, Year {year}, up to {inference_date}\")\n",
        "    print(f\"Mean predicted yield: {np.mean(predicted_yields):.2f}\")\n",
        "    print(f\"Std Dev of predicted yield: {np.std(predicted_yields):.2f}\")\n",
        "\n",
        "    return predicted_yields, bin_edges, hist_counts\n",
        "\n",
        "# --- How to use the inference function ---\n",
        "# Assume 'trained_model' and 'fips_to_id' mapping are available after training\n",
        "\n",
        "# Example Usage:\n",
        "# data_lake_root = \"../data/data_lake_organized\" # Set your data root path\n",
        "# target_year = 2023 # The year you want to predict for\n",
        "# target_county_fips = \"17031\" # Example FIPS (Cook County, IL - likely not a corn county, use a real one)\n",
        "# target_crop = \"corn\"\n",
        "# current_date = \"2023-06-15\" # Date up to which weather is available\n",
        "\n",
        "# Make sure you have the fips_to_id mapping from the dataset object used for training\n",
        "# Example: fips_to_id_mapping_from_training = dataset.get_fips_mapping()[0]\n",
        "\n",
        "\n",
        "# predicted_yields_samples, bins, hist_counts = predict_distribution(\n",
        "#     trained_model,\n",
        "#     data_lake_root,\n",
        "#     target_year,\n",
        "#     target_county_fips,\n",
        "#     target_crop,\n",
        "#     current_date,\n",
        "#     fips_to_id_mapping_from_training, # Pass the mapping\n",
        "#     num_samples=200, # More samples for better histogram\n",
        "#     device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# )\n",
        "\n",
        "# if predicted_yields_samples is not None:\n",
        "#     # You can plot the histogram\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     plt.bar(bins[:-1], hist_counts, width=np.diff(bins), edgecolor='black', alpha=0.7)\n",
        "#     plt.xlabel(\"Predicted Yield (bushels)\")\n",
        "#     plt.ylabel(\"Frequency (Count of Samples)\")\n",
        "#     plt.title(f\"Predicted Yield Distribution for {target_crop.capitalize()} in FIPS {target_county_fips}, Year {target_year} (Data up to {current_date})\")\n",
        "#     plt.grid(axis='y', alpha=0.5)\n",
        "#     plt.show()\n",
        "\n",
        "#     # Or print histogram data\n",
        "#     print(\"\\nHistogram Data:\")\n",
        "#     for i in range(len(hist_counts)):\n",
        "#         print(f\"  Yield range: {bins[i]:.2f} - {bins[i+1]:.2f}, Count: {hist_counts[i]}\")"
      ],
      "metadata": {
        "id": "TChNnnQ0zlfQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}