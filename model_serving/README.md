# AgriYield: Crop Yield Prediction System - Model Serving & Ecosystem

## 1. Overview

This document details the **Model Serving** component and its interaction with other key services within the AgriYield Crop Yield Prediction System. The system is designed to predict crop yield histograms based on county, date, and crop type, leveraging machine learning models and providing a robust serving and monitoring infrastructure.

**Author:** Ali Hamza

**Primary Responsibilities:** Model Serving, Evaluation, Monitoring

## 2. System Architecture and Components

The AgriYield system comprises several interconnected services, orchestrated via Docker Compose (`docker-compose.yml`).

### 2.1. Model Serving (`model_serving/`)

This is the core component responsible for exposing trained ML models via a REST API to provide on-demand yield histogram predictions.

*   **Purpose:** Serve histogram predictions, manage model loading (including MLflow integration and dummy fallbacks), and provide health/information endpoints.
*   **Framework:** FastAPI
*   **Key Files & Directories:**
    *   `app.py`: Main FastAPI application, defines API endpoints, request/response handling, and Prometheus instrumentation.
    *   `predict.py`: Contains the core prediction logic (`predict_yield`, `predict_yield_batch`), including feature fetching, model input preparation, inference, and output processing.
    *   `mlflow_loader.py`: Manages loading models and associated mappings (FIPS, crop IDs). It attempts to load from a configured MLflow instance and can fall back to a local dummy model (`DummyLSTMTCNHistogramPredictor`) if MLflow is unavailable or `dummy_mode` is enabled.
    *   `schemas.py`: Defines Pydantic models for API request (`PredictionRequest`) and response (`BatchPredictionResponseItem`) validation and serialization.
    *   `drift.py`: Contains foundational logic for data drift detection (`log_prediction_for_monitoring`, placeholder `check_data_drift`). `log_prediction_for_monitoring` is called to log request/response pairs.
    *   `Dockerfile`: Defines the Docker image for the model serving application.
    *   `entrypoint.sh`: Script run at container startup. Critically, it calls `model_training/generate_dummy_assets.py` to ensure dummy model artifacts (state dict, mappings) are available for the fallback mechanism.
    *   `pyproject.toml` & `poetry.lock`: Python dependency management.
*   **API Endpoints (defined in `app.py`):**
    *   `/predict` (POST): Accepts a single `PredictionRequest` and returns a histogram prediction.
    *   `/health` (GET): Provides a health check for the service.
    *   `/metrics` (GET): Exposes Prometheus metrics.
*   **Dummy Mode & Fallback:**
    *   If the `MLFLOW_RUN_ID` is not set/MLflow is unavailable, or if `MLFLOW_DUMMY_MODE` is explicitly `true`, the service loads a `DummyLSTMTCNHistogramPredictor` (from `model_training/model.py`).
    *   Dummy assets (model state and mappings) are generated by `model_training/generate_dummy_assets.py` (run by `entrypoint.sh`) and stored in paths specified by environment variables (e.g., `MLFLOW_DUMMY_MODEL_PATH`).
    *   The number of bins for the dummy model is configurable via `MLFLOW_DUMMY_MODEL_NUM_BINS` (default: 5).
*   **Dependencies:** `feature-serving` (for real-time features), MLflow (optional, for real models), `model_training` (for dummy model class and asset generation script).

### 2.2. Feature Serving (`feature_serving/`)

*   **Purpose:** Provides historical and real-time features (primarily weather data) required for model inference.
*   **Framework:** FastAPI
*   **Key Files:**
    *   `main.py`: FastAPI application, defines the `/features` endpoint.
    *   `data_loader.py`: Handles fetching data from data sources (e.g., Swift object storage). Includes a dummy mode to return mock weather data if the primary data source is unavailable or if a "DUMMY" FIPS code is requested.
*   **API Endpoint:** `/features` (GET): Returns features for a given county, year, cut-off date, and crop.
*   **Dependencies:** External data storage (e.g., Swift). If unavailable, relies on its internal dummy data generation.

### 2.3. Model Training (`model_training/`)

*   **Purpose:** Contains scripts and code for training yield prediction models. While the full training pipeline might be more extensive and potentially run outside this service structure (e.g., on dedicated training infrastructure), this directory holds key model definitions.
*   **Key Files:**
    *   `model.py`: Defines the neural network architectures, including `LSTMTCNHistogramPredictor` (for actual histogram prediction) and `DummyLSTMTCNHistogramPredictor` (used by `model-serving` for fallback).
    *   `generate_dummy_assets.py`: A utility script used by `model-serving`'s `entrypoint.sh` to create dummy model files (`dummy_model_state.pth`) and mapping files (`fips_to_id_mapping.json`, `crop_to_id_mapping.json`) if they don't exist. This ensures `model-serving` can always start up in dummy mode.

### 2.4. Dashboard (`dashboard/`)

*   **Purpose:** Provides a web-based user interface for interacting with the `model-serving` API to get and visualize yield histogram predictions.
*   **Framework:** Streamlit
*   **Key Files:**
    *   `app.py`: The main Streamlit application. Provides UI elements for users to input FIPS code, cut-off date, crop type, and (optionally) custom histogram bin edges.
    *   `api_service.py`: Contains functions (`fetch_single_crop_histogram_prediction`) to make requests to the `model-serving` API's `/predict` endpoint.
    *   `data_processor.py`: Processes the JSON response from `model-serving` into a format suitable for `st.bar_chart`.
*   **Interaction:** The dashboard sends requests to `model-serving` (URL configurable via `MODEL_SERVING_API_URL` env var, defaults to `http://model-serving:8000`).
*   **Prometheus Metrics:** Exposes its own Prometheus metrics (e.g., page views, prediction requests) on port `9095`.

### 2.5. Monitoring (`monitoring/`)

This setup provides observability into the system's services.

*   **Prometheus:**
    *   **Configuration:** `monitoring/prometheus/prometheus.yml` defines scrape targets.
    *   **Targets:** Scrapes metrics from:
        *   `model-serving` (`/metrics`)
        *   `feature-serving` (`/metrics`)
        *   `dashboard` (port `9095`)
    *   **Instrumentation:**
        *   FastAPI services (`model-serving`, `feature-serving`) use `prometheus-fastapi-instrumentator`.
        *   The Streamlit `dashboard` uses `prometheus-client` for custom metrics.
*   **Grafana:**
    *   **Purpose:** Visualizes metrics collected by Prometheus.
    *   **Provisioning:**
        *   Datasource: `monitoring/grafana/provisioning/datasources/prometheus.yml` (auto-configures Prometheus datasource).
        *   Dashboard: `monitoring/grafana/provisioning/dashboards/default.json` (provides pre-built panels for key metrics like request rates, latency, error rates for services, and dashboard activity).
*   **Locust (Load Testing):**
    *   **Purpose:** Perform load testing against `model-serving` and `feature-serving` APIs.
    *   **Files:**
        *   `monitoring/locustfile.py`: Defines user behaviors for testing `/predict` (model-serving) and `/features` (feature-serving).
        *   `monitoring/Dockerfile` & `monitoring/requirements.txt`: For building a dedicated Locust Docker service.
    *   **Execution:** Run as part of a "testing" profile in `docker-compose.yml`.

### 2.6. Evaluation Aspects

*   **Responsibility:** While there isn't a dedicated "evaluation service," the design incorporates elements for model and system evaluation, primarily within `model-serving`.
*   **Data Drift Monitoring:**
    *   The `model_serving/drift.py` module includes `log_prediction_for_monitoring`, which is called by `model_serving/predict.py` to record prediction inputs and outputs. This data can be used for offline analysis of data drift or model performance degradation.
    *   The `check_data_drift` function is a placeholder, suggesting a potential area for future development of automated drift detection.
*   **Performance Metrics:** Prometheus metrics (latency, error rates) provide a continuous view of operational performance.
*   **Load Testing:** Locust setup allows for evaluating system performance under load.

## 3. Dependencies and Current State Notes

*   **External Model Dependencies:** For production/real predictions, `model-serving` relies on models logged to an accessible MLflow instance. If MLflow is not configured or the specified model is unavailable, it falls back to the dummy model.
*   **External Data Dependencies:** `feature-serving` requires access to its configured data source (e.g., Swift) for real features. If this is unavailable, it will return dummy feature data.
*   **Training Data & Pipeline:** The full ML model training pipeline and associated datasets are considered external to this core serving/monitoring setup. The `model_training/` directory primarily provides model class definitions and utilities for the dummy mode.
*   **Data Drift Automation:** The framework for logging predictions for drift analysis is in place (`model_serving/drift.py`). However, automated drift detection and alerting mechanisms based on `check_data_drift` would be a further enhancement.
*   **User-Provided Bins:** The dashboard allows users to specify histogram bin edges. Users must ensure the number of bins implied by these edges (number of edges - 1) matches the output dimension of the model being queried (e.g., 5 for the default dummy model). Mismatches will result in an API error from `model-serving`.

This README provides a snapshot of the `model-serving` component and its ecosystem. For detailed execution instructions, refer to the main project `README.md` and `docker-compose.yml` files.
