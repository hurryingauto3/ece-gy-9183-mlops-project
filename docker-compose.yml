services:
  mlflow:
    profiles: ["services"]
    image: ghcr.io/mlflow/mlflow:v2.0.1
    container_name: mlflow-container
    platform: linux/amd64 # CHANGE to linux/arm64/v8 if your host is ARM64 (uname -m is aarch64)
                          # CHANGE to linux/arm64/v8 if your host is ARM64 (uname -m is aarch64)
    ports:
      # IMPORTANT: Your .env has MLFLOW_TRACKING_URI=http://mlflow:5000
      # Your compose maps 5001:5001.
      # To be consistent and use the compose port, change MLFLOW_TRACKING_URI in .env to 5001
      # Or change this ports mapping to "5000:5000" and the --port below to 5000.
      # Let's assume you change .env to 5001 for now.
      - "5001:5001"
    environment:
      # Pass standard OpenStack OS_* variables required by openstack.connect() / MLflow Swift
      # using Application Credentials
      - OS_AUTH_URL=${OS_AUTH_URL} # From .env
      - OS_AUTH_TYPE=${OS_AUTH_TYPE}
      - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID} # From .env
      - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET} # From .env
      # Include Project ID and Region Name, as they are still relevant
      # - OS_PROJECT_ID=${OS_PROJECT_ID} # From .env - uncomment if needed by SDK/Swift backend
      - OS_REGION_NAME=${OS_REGION_NAME} # From .env (Optional)

      # MLflow Swift specific settings (mostly stay the same)
      - MLFLOW_S3_ENDPOINT_URL= # Leave blank for OpenStack
      - MLFLOW_S3_IGNORE_TLS=true
      # - MLFLOW_S3_REGION_NAME=${OS_REGION_NAME} # Can use OS_REGION_NAME here if needed

      # Backend store (metadata) - stays the same
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/backend/mlflow.db
      # Point artifacts to Swift - stays the same (used by command line)
      - MLFLOW_ARTIFACT_LOCATION=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}/mlflow-artifacts # Keep just the path part here

    volumes:
      # The backend store and artifact root locations reference these mounted volumes
      - mlflow_tracking_data:/mlflow/backend
      - mlflow_artifact_data:/mlflow/artifacts # This volume is less relevant for Swift artifacts, can be removed if not needed for local logging

    restart: unless-stopped

    # ADD THIS COMMAND BLOCK
    command:
      - mlflow
      - server
      - --backend-store-uri # Flag for the backend store
      - sqlite:///mlflow/backend/mlflow.db # Value pointing to the sqlite file in the volume
      - --default-artifact-root # CORRECT FLAG for artifact root in this MLflow version
      - swift://${FS_OPENSTACK_SWIFT_CONTAINER_NAME}/mlflow-artifacts # Value pointing to the Swift location using the env var
      - --host # Flag for the host to listen on
      - 0.0.0.0 # Listen on all interfaces
      - --port # Flag for the port to listen on
      - "5001" # Use port 5001 to match the ports mapping in compose

  # --- Feature Service ---
  feature-service:
    profiles: ["services", "gpu"]
    build:
      context: ./feature_serving # Corrected context
      dockerfile: Dockerfile
    container_name: feature-service-container
    ports:
      - target: 8001
        published: 8001
        protocol: tcp
        mode: ingress
    restart: unless-stopped
    environment:
      # Pass standard OpenStack OS_* variables required by openstack.connect()
      # using Application Credentials
      - OS_AUTH_URL=${OS_AUTH_URL}
      - OS_AUTH_TYPE=${OS_AUTH_TYPE}
      - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID}
      - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET}
      # Include Project ID and Region Name
      # - OS_PROJECT_ID=${OS_PROJECT_ID}
      - OS_REGION_NAME=${OS_REGION_NAME} # Optional

      # Your script specific variable (Swift container name)
      - FS_OPENSTACK_SWIFT_CONTAINER_NAME=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}

      # Optional log level
      # - LOG_LEVEL=${LOG_LEVEL:-INFO}

  # --- Model Serving Service ---
  model-serving:
    profiles: ["services"]
    build:
      context: .
      dockerfile: model_serving/Dockerfile
    container_name: model-serving-container
    ports:
      - "8000:8000"
    environment:
      - FEATURE_SERVICE_URL=http://feature-service:8001
      - MLFLOW_TRACKING_URI=http://mlflow:5001
      - MLFLOW_MODEL_NAME=${MLFLOW_MODEL_NAME}
      - MLFLOW_MODEL_STAGE=${MLFLOW_MODEL_STAGE}
      
      # --- MLflow Dummy Mode Configuration (for fallback if MLflow is unavailable) ---
      # DUMMY_MODE is false by default; fallback to dummy assets happens if MLflow load fails.
      # Set MLFLOW_DUMMY_MODE=true in .env to force dummy mode.
      - MLFLOW_DUMMY_MODE=${MLFLOW_DUMMY_MODE:-true}
      # Paths for assets generated INSIDE the container by entrypoint.sh
      - MLFLOW_DUMMY_MODEL_PATH=/app/dummy_assets/dummy_model_state.pth
      - MLFLOW_DUMMY_FIPS_MAPPING_PATH=/app/dummy_assets/fips_to_id_mapping.json
      - MLFLOW_DUMMY_CROP_MAPPING_PATH=/app/dummy_assets/crop_to_id_mapping.json
      - MLFLOW_DUMMY_MODEL_NUM_BINS=${MLFLOW_DUMMY_MODEL_NUM_BINS:-5} # Can be overridden by .env
      # Directory for the asset generation script
      - DUMMY_ASSETS_DIR=/app/dummy_assets

      # API Settings
      - API_PREDICT_LIMIT=${API_PREDICT_LIMIT}
      - API_PREDICT_BATCH_LIMIT=${API_PREDICT_BATCH_LIMIT}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    restart: unless-stopped
    depends_on:
      - feature-service
      - mlflow

  # --- Prometheus Service ---
  prometheus:
    profiles: ["services"]
    image: prom/prometheus:latest
    container_name: prometheus-container
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    restart: unless-stopped
    depends_on:
      - model-serving

  # --- Grafana Service ---
  grafana:
    profiles: ["services"]  
    image: grafana/grafana:latest
    container_name: grafana-container
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - grafana_data:/var/lib/grafana
    restart: unless-stopped
    depends_on:
      - prometheus
      - mlflow  # Added dependency on mlflow service

  # --- Streamlit Dashboard Service ---
  dashboard:
    profiles: ["services"]
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    container_name: dashboard-container
    ports:
      - "8501:8501"
    environment:
      - MODEL_SERVING_API_URL=http://model-serving:8000
      # You can add other necessary environment variables for the dashboard here
      # For example, if you want to control Streamlit's log level:
      # - STREAMLIT_LOG_LEVEL=debug 
    restart: unless-stopped
    depends_on:
      - model-serving # Dashboard needs the model-serving API to be available

  # --- Locust Load Testing Service ---
  locust-service:
    profiles: ["testing"] # Only run when explicitly requested
    build:
      context: ./monitoring
      dockerfile: Dockerfile
    container_name: locust-container
    ports:
      - "8089:8089" # Locust web UI
    # The locustfile.py inside the container is configured to use 
    # http://model-serving:8000 and http://feature-service:8001 as hosts.
    # Ensure this service is on the same network as model-serving and feature-service.
    # Docker Compose typically handles this by default by creating a default network.
    depends_on:
      - model-serving
      - feature-serving
    # command: -f /mnt/locust/locustfile.py --headless -u 10 -r 2 --run-time 1m # Example headless run

# --- ETL Job Service ---

  # etl-download:
  #   build: ./etl_pipeline/extract
  #   container_name: etl-extract-container
  #   volumes:
  #     # - ./local_data:/app/data_lake
  #     # - /mnt/persistent/data_lake:/app/data_lake  # use persistent storage
  #     - /home/cc/swift_s3:/mnt/swift_store
  #   # Add this command section:
  #   command:
  #     - poetry
  #     - run
  #     - python # Or /usr/local/bin/python3 depending on your base image/install
  #     - /app/drive_to_swift_etl.py # Path to your script inside the container
  #     - ${GOOGLE_DRIVE_FOLDER_ID_HRRR} # Positional argument folder_id
  #     - ${GOOGLE_DRIVE_FOLDER_ID_USDA} 
  #     # - --swift-container # Named argument flag
  #     # - ${FS_OPENSTACK_SWIFT_CONTAINER_NAME} # Named argument value
  #     - --clean # Optional: Add --clean flag if you want it to clean up locally

  #   environment:
  #     # Explicitly pass OpenStack credentials to the container
  #     # These are needed by the openstack SDK within the script
  #     - OS_AUTH_URL=${OS_AUTH_URL}
  #     - OS_AUTH_TYPE=${OS_AUTH_TYPE}
  #     - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID}
  #     - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET}
  #     - OS_REGION_NAME=${OS_REGION_NAME}
  #     # Pass the container name as env var as well, useful for debugging
  #     # although the script reads it from command line via compose args
  #     # - FS_OPENSTACK_SWIFT_CONTAINER_NAME=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}
  #     # Pass the folder ID as env var as well, useful for debugging
  #     - GOOGLE_DRIVE_FOLDER_ID_HRRR=${GOOGLE_DRIVE_FOLDER_ID_HRRR}
  #     - GOOGLE_DRIVE_FOLDER_ID_USDA=${GOOGLE_DRIVE_FOLDER_ID_USDA}

  # etl-transform:
  #   profiles: ["gpu"]
  #   build: ./etl_pipeline/transform
  #   container_name: etl-transform-container
  #   volumes:
  #     # - ./local_data:/app/data_lake  # Fetch data from local VM storage
  #     # - persistent_data:/mnt/project4/data
  #     - /mnt/persistent/data_lake:/app/data_lake
  #     - /mnt/persistent/project4/data:/mnt/project4/data
  #   # Add this command section:
  #   command:
  #     - poetry
  #     - run
  #     - python # Or /usr/local/bin/python3 depending on your base image/install
  #     - /app/transform.py # Path to your script inside the container
  #     - --swift-container # Named argument flag
  #     - ${FS_OPENSTACK_SWIFT_CONTAINER_NAME} # Named argument value
  #     - --clean # Optional: Add --clean flag if you want it to clean up locally

  #   environment:
  #     # Explicitly pass OpenStack credentials to the container
  #     # These are needed by the openstack SDK within the script
  #     - OS_AUTH_URL=${OS_AUTH_URL}
  #     - OS_AUTH_TYPE=${OS_AUTH_TYPE}
  #     - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID}
  #     - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET}
  #     - OS_REGION_NAME=${OS_REGION_NAME}
  #     # Pass the container name as env var as well, useful for debugging
  #     # although the script reads it from command line via compose args
  #     - FS_OPENSTACK_SWIFT_CONTAINER_NAME=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}
  #     # Pass the folder ID as env var as well, useful for debugging


  # --- Model Training Job Service ---
  # model-training:
  #   profiles: ["gpu"]
  #   build:
  #     context: ./model_training
  #     dockerfile: Dockerfile
  #   container_name: model-training-container
  #   restart: "no"
  #   environment:
  #     # MLflow config
  #     - MLFLOW_TRACKING_URI=http://mlflow:5001
  #     - MLFLOW_MODEL_NAME=${MLFLOW_MODEL_NAME}
  #     - MLFLOW_MODEL_STAGE=${MLFLOW_MODEL_STAGE}
  #     - MLFLOW_EXPERIMENT_NAME=${MLFLOW_EXPERIMENT_NAME}

  #     # Pass standard OpenStack OS_* variables required by openstack.connect() (for data loading)
  #     # using Application Credentials
  #     - OS_AUTH_URL=${OS_AUTH_URL}
  #     - OS_AUTH_TYPE=${OS_AUTH_TYPE}
  #     - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID}
  #     - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET}
  #     # Include Project ID and Region Name
  #     # - OS_PROJECT_ID=${OS_PROJECT_ID}
  #     - OS_REGION_NAME=${OS_REGION_NAME} # Optional

  #     # Your script specific variable (Swift container name for data)
  #     - FS_OPENSTACK_SWIFT_CONTAINER_NAME=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}

  #     # Pass training hyperparameters (use placeholders reading from .env or shell)
  #     # Defaults are set in train_job.py, but explicit variables are clearer
  #     - TRAIN_EPOCHS=${TRAIN_EPOCHS:-50}
  #     - TRAIN_LR=${TRAIN_LR:-1e-3}
  #     - TRAIN_BATCH_SIZE=${TRAIN_BATCH_SIZE:-32}
  #     - TRAIN_FIPS_EMBEDDING_DIM=${TRAIN_FIPS_EMBEDDING_DIM:-16}
  #     - TRAIN_HIDDEN_DIM=${TRAIN_HIDDEN_DIM:-64}
  #     - TRAIN_LSTM_LAYERS=${TRAIN_LSTM_LAYERS:-1}
  #     - TRAIN_TCN_CHANNELS=${TRAIN_TCN_CHANNELS:-64,32}
  #     - TRAIN_DROPOUT_RATE=${TRAIN_DROPOUT_RATE:-0.1}
  #     - TRAIN_HOLDOUT_YEAR=${TRAIN_HOLDOUT_YEAR:-}
  #     - TRAIN_VAL_YEAR_RATIO=${TRAIN_VAL_YEAR_RATIO:-0.2}
  #     - TRAIN_CROP_NAME=${TRAIN_CROP_NAME:-corn}


# Define named volumes for data persistence
volumes:
  mlflow_tracking_data:
  mlflow_artifact_data:
  prometheus_data:
  grafana_data:
  persistent_data:

