services:

  # --- New MLflow Service ---
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.0.1
    container_name: mlflow-container
    platform: linux/arm64/v8 
    ports:
      - "5001:5001"
    environment:
      # Pass standard OpenStack OS_* variables required by openstack.connect() / MLflow Swift
      # using Application Credentials
      - OS_AUTH_URL=${OS_AUTH_URL} # From .env
      - OS_AUTH_TYPE=${OS_AUTH_TYPE}
      - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID} # From .env
      - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET} # From .env
      # Include Project ID and Region Name, as they are still relevant
      # - OS_PROJECT_ID=${OS_PROJECT_ID} # From .env
      - OS_REGION_NAME=${OS_REGION_NAME} # From .env (Optional)

      # MLflow Swift specific settings (mostly stay the same)
      - MLFLOW_S3_ENDPOINT_URL= # Leave blank for OpenStack
      - MLFLOW_S3_IGNORE_TLS=true
      # - MLFLOW_S3_REGION_NAME=${OS_REGION_NAME} # Can use OS_REGION_NAME here if needed

      # Backend store (metadata) - stays the same
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/backend/mlflow.db
      # Point artifacts to Swift - stays the same
      - MLFLOW_ARTIFACT_LOCATION=swift://${FS_OPENSTACK_SWIFT_CONTAINER_NAME}/mlflow-artifacts

    volumes:
      - mlflow_tracking_data:/mlflow/backend
      - mlflow_artifact_data:/mlflow/artifacts

    restart: unless-stopped

  # --- ETL Job Service ---
  etl:
    build: ./etl_pipeline
    container_name: etl-container
    # Add this command section:
    command:
      - poetry
      - run
      - python # Or /usr/local/bin/python3 depending on your base image/install
      - /app/drive_to_swift_etl.py # Path to your script inside the container
      - ${GOOGLE_DRIVE_FOLDER_ID} # Positional argument folder_id
      - --swift-container # Named argument flag
      - ${FS_OPENSTACK_SWIFT_CONTAINER_NAME} # Named argument value
      - --clean # Optional: Add --clean flag if you want it to clean up locally

    environment:
      # Explicitly pass OpenStack credentials to the container
      # These are needed by the openstack SDK within the script
      - OS_AUTH_URL=${OS_AUTH_URL}
      - OS_AUTH_TYPE=${OS_AUTH_TYPE}
      - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID}
      - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET}
      - OS_REGION_NAME=${OS_REGION_NAME}
      # Pass the container name as env var as well, useful for debugging
      # although the script reads it from command line via compose args
      - FS_OPENSTACK_SWIFT_CONTAINER_NAME=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}
      # Pass the folder ID as env var as well, useful for debugging
      - GOOGLE_DRIVE_FOLDER_ID=${GOOGLE_DRIVE_FOLDER_ID}


  # --- Model Training Job Service ---
  model-training:
    build:
      context: ./model_training
      dockerfile: Dockerfile
    container_name: model-training-container
    restart: "no"
    environment:
      # MLflow config
      - MLFLOW_TRACKING_URI=http://mlflow:5001
      - MLFLOW_MODEL_NAME=${MLFLOW_MODEL_NAME}
      - MLFLOW_MODEL_STAGE=${MLFLOW_MODEL_STAGE}
      - MLFLOW_EXPERIMENT_NAME=${MLFLOW_EXPERIMENT_NAME}

      # Pass standard OpenStack OS_* variables required by openstack.connect() (for data loading)
      # using Application Credentials
      - OS_AUTH_URL=${OS_AUTH_URL}
      - OS_AUTH_TYPE=${OS_AUTH_TYPE}
      - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID}
      - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET}
      # Include Project ID and Region Name
      # - OS_PROJECT_ID=${OS_PROJECT_ID}
      - OS_REGION_NAME=${OS_REGION_NAME} # Optional

      # Your script specific variable (Swift container name for data)
      - FS_OPENSTACK_SWIFT_CONTAINER_NAME=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}

      # Pass training hyperparameters (use placeholders reading from .env or shell)
      # Defaults are set in train_job.py, but explicit variables are clearer
      - TRAIN_EPOCHS=${TRAIN_EPOCHS:-50}
      - TRAIN_LR=${TRAIN_LR:-1e-3}
      - TRAIN_BATCH_SIZE=${TRAIN_BATCH_SIZE:-32}
      - TRAIN_FIPS_EMBEDDING_DIM=${TRAIN_FIPS_EMBEDDING_DIM:-16}
      - TRAIN_HIDDEN_DIM=${TRAIN_HIDDEN_DIM:-64}
      - TRAIN_LSTM_LAYERS=${TRAIN_LSTM_LAYERS:-1}
      - TRAIN_TCN_CHANNELS=${TRAIN_TCN_CHANNELS:-64,32}
      - TRAIN_DROPOUT_RATE=${TRAIN_DROPOUT_RATE:-0.1}
      - TRAIN_HOLDOUT_YEAR=${TRAIN_HOLDOUT_YEAR:-}
      - TRAIN_VAL_YEAR_RATIO=${TRAIN_VAL_YEAR_RATIO:-0.2}
      - TRAIN_CROP_NAME=${TRAIN_CROP_NAME:-corn}


  # --- Feature Service ---
  feature-service:
    build:
      context: ./feature_serving # Corrected context
      dockerfile: Dockerfile
    container_name: feature-service-container
    ports:
      - target: 8001
        published: 8001
        protocol: tcp
        mode: ingress
    restart: unless-stopped
    environment:
      # Pass standard OpenStack OS_* variables required by openstack.connect()
      # using Application Credentials
      - OS_AUTH_URL=${OS_AUTH_URL}
      - OS_AUTH_TYPE=${OS_AUTH_TYPE}
      - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID}
      - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET}
      # Include Project ID and Region Name
      # - OS_PROJECT_ID=${OS_PROJECT_ID}
      - OS_REGION_NAME=${OS_REGION_NAME} # Optional

      # Your script specific variable (Swift container name)
      - FS_OPENSTACK_SWIFT_CONTAINER_NAME=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}

      # Optional log level
      # - LOG_LEVEL=${LOG_LEVEL:-INFO}


  # --- Model Serving Service ---
  model-serving:
    build:
      context: ./model_serving
      dockerfile: Dockerfile
    container_name: model-serving-container
    ports:
      - "8000:8000"
    environment:
      - FEATURE_SERVICE_URL=http://feature-service:8001
      - MLFLOW_TRACKING_URI=http://mlflow:5001
      - MLFLOW_MODEL_NAME=${MLFLOW_MODEL_NAME}
      - MLFLOW_MODEL_STAGE=${MLFLOW_MODEL_STAGE}
      # API Settings
      - API_PREDICT_LIMIT=${API_PREDICT_LIMIT}
      - API_PREDICT_BATCH_LIMIT=${API_PREDICT_BATCH_LIMIT}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    restart: unless-stopped
    depends_on:
      - feature-service
      - mlflow

  # --- Prometheus Service ---
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus-container
    ports:
      - "9090:9090"
    volumes:
      - ./devops/docker-compose/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    restart: unless-stopped
    depends_on:
      - model-serving

  # --- Grafana Service ---
  grafana:
    image: grafana/grafana:latest
    container_name: grafana-container
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
    volumes:
      - ./devops/docker-compose/grafana/provisioning:/etc/grafana/provisioning
      - ./devops/docker-compose/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - grafana_data:/var/lib/grafana
    restart: unless-stopped
    depends_on:
      - prometheus

# Define named volumes for data persistence
volumes:
  mlflow_tracking_data:
  mlflow_artifact_data:
  prometheus_data:
  grafana_data: