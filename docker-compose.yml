services:
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile.jupyter
    image: jupyter-mlflow
    container_name: jupyter-container
    ports:
      - "0.0.0.0:8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5001
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.ip=0.0.0.0 --NotebookApp.allow_origin='*'
    restart: unless-stopped
    depends_on:
      - mlflow
  # --- New MLflow Service ---
    # --- New MLflow Service ---
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.0.1
    container_name: mlflow-container
    platform: linux/amd64 # KEEP this if your host is AMD64 (uname -m is x86_64)
                          # CHANGE to linux/arm64/v8 if your host is ARM64 (uname -m is aarch64)
    ports:
      # IMPORTANT: Your .env has MLFLOW_TRACKING_URI=http://mlflow:5000
      # Your compose maps 5001:5001.
      # To be consistent and use the compose port, change MLFLOW_TRACKING_URI in .env to 5001
      # Or change this ports mapping to "5000:5000" and the --port below to 5000.
      # Let's assume you change .env to 5001 for now.
      - "5001:5001"
    environment:
      # Pass standard OpenStack OS_* variables required by openstack.connect() / MLflow Swift
      # using Application Credentials
      - OS_AUTH_URL=${OS_AUTH_URL} # From .env
      - OS_AUTH_TYPE=${OS_AUTH_TYPE}
      - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID} # From .env
      - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET} # From .env
      # Include Project ID and Region Name, as they are still relevant
      # - OS_PROJECT_ID=${OS_PROJECT_ID} # From .env - uncomment if needed by SDK/Swift backend
      - OS_REGION_NAME=${OS_REGION_NAME} # From .env (Optional)

      # MLflow Swift specific settings (mostly stay the same)
      - MLFLOW_S3_ENDPOINT_URL= # Leave blank for OpenStack
      - MLFLOW_S3_IGNORE_TLS=true
      # - MLFLOW_S3_REGION_NAME=${OS_REGION_NAME} # Can use OS_REGION_NAME here if needed

      # Backend store (metadata) - stays the same
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/backend/mlflow.db
      # Point artifacts to Swift - stays the same (used by command line)
      - MLFLOW_ARTIFACT_LOCATION=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}/mlflow-artifacts # Keep just the path part here

    volumes:
      # The backend store and artifact root locations reference these mounted volumes
      - mlflow_tracking_data:/mlflow/backend
      - mlflow_artifact_data:/mlflow/artifacts # This volume is less relevant for Swift artifacts, can be removed if not needed for local logging

    restart: unless-stopped

    # ADD THIS COMMAND BLOCK
    command:
      - mlflow
      - server
      - --backend-store-uri # Flag for the backend store
      - sqlite:///mlflow/backend/mlflow.db # Value pointing to the sqlite file in the volume
      - --default-artifact-root # CORRECT FLAG for artifact root in this MLflow version
      - swift://${FS_OPENSTACK_SWIFT_CONTAINER_NAME}/mlflow-artifacts # Value pointing to the Swift location using the env var
      - --host # Flag for the host to listen on
      - 0.0.0.0 # Listen on all interfaces
      - --port # Flag for the port to listen on
      - "5001" # Use port 5001 to match the ports mapping in compose

  # --- ETL Job Service ---
  etl-download:
    build: ./etl_pipeline/extract
    container_name: etl-extract-container
    volumes:
      - ./local_data:/app/data_lake 
    # Add this command section:
    command:
      - poetry
      - run
      - python # Or /usr/local/bin/python3 depending on your base image/install
      - /app/drive_to_swift_etl.py # Path to your script inside the container
      - ${GOOGLE_DRIVE_FOLDER_ID_HRRR} # Positional argument folder_id
      - ${GOOGLE_DRIVE_FOLDER_ID_USDA} 
      # - --swift-container # Named argument flag
      # - ${FS_OPENSTACK_SWIFT_CONTAINER_NAME} # Named argument value
      - --clean # Optional: Add --clean flag if you want it to clean up locally

    environment:
      # Explicitly pass OpenStack credentials to the container
      # These are needed by the openstack SDK within the script
      - OS_AUTH_URL=${OS_AUTH_URL}
      - OS_AUTH_TYPE=${OS_AUTH_TYPE}
      - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID}
      - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET}
      - OS_REGION_NAME=${OS_REGION_NAME}
      # Pass the container name as env var as well, useful for debugging
      # although the script reads it from command line via compose args
      # - FS_OPENSTACK_SWIFT_CONTAINER_NAME=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}
      # Pass the folder ID as env var as well, useful for debugging
      - GOOGLE_DRIVE_FOLDER_ID_HRRR=${GOOGLE_DRIVE_FOLDER_ID_HRRR}
      - GOOGLE_DRIVE_FOLDER_ID_USDA=${GOOGLE_DRIVE_FOLDER_ID_USDA}

  etl-transform:
    build: ./etl_pipeline/transform
    container_name: etl-transform-container
    volumes:
      - ./local_data:/app/data_lake  # Fetch data from local VM storage
      - persistent_data:/mnt/project4/data
    # Add this command section:
    command:
      - poetry
      - run
      - python # Or /usr/local/bin/python3 depending on your base image/install
      - /app/transform.py # Path to your script inside the container
      - --swift-container # Named argument flag
      - ${FS_OPENSTACK_SWIFT_CONTAINER_NAME} # Named argument value
      - --clean # Optional: Add --clean flag if you want it to clean up locally

    environment:
      # Explicitly pass OpenStack credentials to the container
      # These are needed by the openstack SDK within the script
      - OS_AUTH_URL=${OS_AUTH_URL}
      - OS_AUTH_TYPE=${OS_AUTH_TYPE}
      - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID}
      - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET}
      - OS_REGION_NAME=${OS_REGION_NAME}
      # Pass the container name as env var as well, useful for debugging
      # although the script reads it from command line via compose args
      - FS_OPENSTACK_SWIFT_CONTAINER_NAME=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}
      # Pass the folder ID as env var as well, useful for debugging


  # --- Model Training Job Service ---
  model-training:
    build:
      context: ./model_training
      dockerfile: Dockerfile
    container_name: model-training-container
    restart: "no"
    environment:
      # MLflow config
      - MLFLOW_TRACKING_URI=http://mlflow:5001
      - MLFLOW_MODEL_NAME=${MLFLOW_MODEL_NAME}
      - MLFLOW_MODEL_STAGE=${MLFLOW_MODEL_STAGE}
      - MLFLOW_EXPERIMENT_NAME=${MLFLOW_EXPERIMENT_NAME}

      # Pass standard OpenStack OS_* variables required by openstack.connect() (for data loading)
      # using Application Credentials
      - OS_AUTH_URL=${OS_AUTH_URL}
      - OS_AUTH_TYPE=${OS_AUTH_TYPE}
      - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID}
      - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET}
      # Include Project ID and Region Name
      # - OS_PROJECT_ID=${OS_PROJECT_ID}
      - OS_REGION_NAME=${OS_REGION_NAME} # Optional

      # Your script specific variable (Swift container name for data)
      - FS_OPENSTACK_SWIFT_CONTAINER_NAME=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}

      # Pass training hyperparameters (use placeholders reading from .env or shell)
      # Defaults are set in train_job.py, but explicit variables are clearer
      - TRAIN_EPOCHS=${TRAIN_EPOCHS:-50}
      - TRAIN_LR=${TRAIN_LR:-1e-3}
      - TRAIN_BATCH_SIZE=${TRAIN_BATCH_SIZE:-32}
      - TRAIN_FIPS_EMBEDDING_DIM=${TRAIN_FIPS_EMBEDDING_DIM:-16}
      - TRAIN_HIDDEN_DIM=${TRAIN_HIDDEN_DIM:-64}
      - TRAIN_LSTM_LAYERS=${TRAIN_LSTM_LAYERS:-1}
      - TRAIN_TCN_CHANNELS=${TRAIN_TCN_CHANNELS:-64,32}
      - TRAIN_DROPOUT_RATE=${TRAIN_DROPOUT_RATE:-0.1}
      - TRAIN_HOLDOUT_YEAR=${TRAIN_HOLDOUT_YEAR:-}
      - TRAIN_VAL_YEAR_RATIO=${TRAIN_VAL_YEAR_RATIO:-0.2}
      - TRAIN_CROP_NAME=${TRAIN_CROP_NAME:-corn}


  # --- Feature Service ---
  feature-service:
    build:
      context: ./feature_serving # Corrected context
      dockerfile: Dockerfile
    container_name: feature-service-container
    ports:
      - target: 8001
        published: 8001
        protocol: tcp
        mode: ingress
    restart: unless-stopped
    environment:
      # Pass standard OpenStack OS_* variables required by openstack.connect()
      # using Application Credentials
      - OS_AUTH_URL=${OS_AUTH_URL}
      - OS_AUTH_TYPE=${OS_AUTH_TYPE}
      - OS_APPLICATION_CREDENTIAL_ID=${OS_APPLICATION_CREDENTIAL_ID}
      - OS_APPLICATION_CREDENTIAL_SECRET=${OS_APPLICATION_CREDENTIAL_SECRET}
      # Include Project ID and Region Name
      # - OS_PROJECT_ID=${OS_PROJECT_ID}
      - OS_REGION_NAME=${OS_REGION_NAME} # Optional

      # Your script specific variable (Swift container name)
      - FS_OPENSTACK_SWIFT_CONTAINER_NAME=${FS_OPENSTACK_SWIFT_CONTAINER_NAME}

      # Optional log level
      # - LOG_LEVEL=${LOG_LEVEL:-INFO}


  # --- Model Serving Service ---
  model-serving:
    build:
      context: ./model_serving
      dockerfile: Dockerfile
    container_name: model-serving-container
    ports:
      - "8000:8000"
    environment:
      - FEATURE_SERVICE_URL=http://feature-service:8001
      - MLFLOW_TRACKING_URI=http://mlflow:5001
      - MLFLOW_MODEL_NAME=${MLFLOW_MODEL_NAME}
      - MLFLOW_MODEL_STAGE=${MLFLOW_MODEL_STAGE}
      # API Settings
      - API_PREDICT_LIMIT=${API_PREDICT_LIMIT}
      - API_PREDICT_BATCH_LIMIT=${API_PREDICT_BATCH_LIMIT}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    restart: unless-stopped
    depends_on:
      - feature-service
      - mlflow

  # --- Prometheus Service ---
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus-container
    ports:
      - "9090:9090"
    volumes:
      - ./devops/docker-compose/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    restart: unless-stopped
    depends_on:
      - model-serving

  # --- Grafana Service ---
  grafana:
    image: grafana/grafana:latest
    container_name: grafana-container
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
    volumes:
      - ./devops/docker-compose/grafana/provisioning:/etc/grafana/provisioning
      - ./devops/docker-compose/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - grafana_data:/var/lib/grafana
    restart: unless-stopped
    depends_on:
      - prometheus

# Define named volumes for data persistence
volumes:
  mlflow_tracking_data:
  mlflow_artifact_data:
  prometheus_data:
  grafana_data:
  persistent_data: